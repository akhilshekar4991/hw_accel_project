{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hash Table Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "from tqdm import tqdm\n",
    "\n",
    "NUM_OF_HASH_BUCKETS = 1024\n",
    "\n",
    "inputFilename = 'dictionary.txt'\n",
    "\n",
    "def convertStrToBucketNum(input_str, NumOfBuckets):\n",
    "    hashobj = hashlib.sha3_512(input_str.encode('utf-8'))\n",
    "    val = int.from_bytes(hashobj.digest(), 'big')\n",
    "    return (val % NumOfBuckets)\n",
    "   \n",
    "# Initialize a hash table\n",
    "myHashTable = {}\n",
    "for num in range(NUM_OF_HASH_BUCKETS):\n",
    "    myHashTable[num] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 349882/349882 [00:01<00:00, 293332.69it/s]\n"
     ]
    }
   ],
   "source": [
    "# Read the files and construct a hash table\n",
    "with open(inputFilename) as file:\n",
    "    Lines = file.readlines()\n",
    "    for item in tqdm(Lines): \n",
    "        bucket_num = convertStrToBucketNum(item, NUM_OF_HASH_BUCKETS)\n",
    "        #print(f'bucket-{bucket_num} = {item}')\n",
    "        #print(myHashTable[bucket_num].append(item))\n",
    "        myHashTable[bucket_num].append(item.rstrip(\"\\n\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2048/2048 [00:00<00:00, 810218.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check TotalLength = 349882\n",
      "avgBucketLength = 170.8408203125\n",
      "maxBucketLength = 214\n",
      "bucketWithLongestLength = 1735\n",
      "['aboral', 'acrochordon', 'adap', 'afterthrift', 'alectorides', 'anaclete', 'anathoth', 'anauli', 'androgenesis', 'artha', 'aspermic', 'asthenia', 'astringency', 'autometric', 'ayyub', 'backy', 'balopticon', 'barcelona', 'baringhaus', 'basch', 'bellefonte', 'bergius', 'beta', 'bimbisara', 'blunge', 'bowermaiden', 'caithness', 'caneware', 'capivi', 'caprinic', 'cascadian', 'cassey', 'ccsu', 'chiffonier', 'chlorines', 'chona', 'chumpaka', 'clonked', 'cofield', 'compromisers', 'conjunctive', 'corsair', 'creeps', 'dallesport', 'darpa', 'debuisson', 'decubital', 'destino', 'developments', 'diarsenide', 'digs', 'diplococcic', 'divorcees', 'doland', 'dolek', 'droller', 'druthers', 'dudicourt', 'dynam', 'electrograph', 'encolumn', 'endways', 'epnm', 'excretions', 'fatidic', 'featurally', 'fireworky', 'ftgdah', 'furuncles', 'galumptious', 'ganglau', 'gatan', 'gaven', 'gecos', 'gerlovo', 'giekes', 'grillroom', 'gymnasiarch', 'hadden', 'hardy', 'headsill', 'hereditarist', 'heteroicous', 'heteronymy', 'histotrophy', 'howlings', 'incisors', 'inerrancy', 'jcst', 'julianne', 'kehoeite', 'kiger', 'kishor', 'lachiguiri', 'lakins', 'larkye', 'lawlants', 'leipon', 'leipzig', 'leiser', 'mahmoud', 'masers', 'massacring', 'massow', 'mesenterical', 'miggs', 'milou', 'minnehaha', 'modulations', 'morglan', 'mucorrhea', 'nameof', 'natchnee', 'neigh', 'newbolds', 'newbould', 'neywick', 'ngindo', 'nurses', 'olympas', 'openplan', 'ovalescent', 'oversalty', 'oxatyo', 'pahokee', 'paintingness', 'palamitism', 'papered', 'paralyzing', 'pdcn', 'peartness', 'pergola', 'peron', 'photofilm', 'pillmaking', 'poissant', 'portoconnor', 'pravity', 'precommit', 'presumptuous', 'previsive', 'protoxide', 'protoypes', 'pushao', 'qualifiedly', 'remix', 'retrofired', 'riyadh', 'ruggedness', 'salway', 'samarra', 'sangerbund', 'sangsue', 'saroua', 'scourfish', 'scowl', 'seditiously', 'serry', 'shavese', 'shiller', 'shipcraft', 'siegecraft', 'sisingga', 'skeldrake', 'slab', 'soullessness', 'speedway', 'spiggoty', 'spurlet', 'squirming', 'stardock', 'staters', 'succedent', 'sulfury', 'sundberg', 'sunlighted', 'sunproof', 'syconid', 'tagant', 'taskbox', 'tegal', 'tierce', 'toucouleur', 'tracings', 'trimstone', 'triverbial', 'uncongenial', 'underschool', 'undispleased', 'unhesitating', 'unitively', 'unperished', 'unpossibly', 'unsalaried', 'unslashed', 'unstoked', 'unsubpoenaed', 'unworshipped', 'valetism', 'veloutine', 'vena', 'verificate', 'verification', 'verley', 'verminly', 'vernality', 'vicianin', 'vilok', 'wichmann', 'witenagemot', 'woodsiest', 'wwwboard', 'zeit', 'zelmo']\n",
      "minBucketLength = 127\n",
      "bucketWithMinLength = \n",
      "['aganice', 'aglet', 'airville', 'altercating', 'anum', 'armisonous', 'arointed', 'attired', 'bandawaminda', 'battiks', 'beseemly', 'blancmanger', 'blankes', 'blaspheming', 'brasscolored', 'buffett', 'bumblings', 'bytownitite', 'caddish', 'cajamarca', 'campton', 'carpolite', 'cashers', 'cementless', 'chanteuse', 'chilasdarel', 'chlorazide', 'chlorinous', 'complin', 'crigger', 'dandizette', 'deatsville', 'despatched', 'dickie', 'discursive', 'dowser', 'doyaayo', 'elset', 'embling', 'emom', 'eneas', 'ensisternum', 'essentia', 'euphorbium', 'eurhodine', 'facemark', 'fashions', 'fluorotype', 'foreshank', 'formyl', 'fracturable', 'ginglymodian', 'gowkit', 'grootaert', 'hatchettine', 'ilonggo', 'impugners', 'inquilinous', 'jaremir', 'jivia', 'kiput', 'knit', 'langett', 'lobber', 'maplerapids', 'mesozoan', 'metaxenia', 'mistrik', 'morphonomic', 'muddish', 'munukania', 'mutase', 'mwraaa', 'nationalista', 'neolite', 'nonpecuniary', 'ogygia', 'passman', 'paytash', 'perchik', 'popi', 'prefine', 'pretexted', 'principle', 'prolatively', 'publishment', 'regname', 'resumer', 'reunionism', 'roachdale', 'scarpine', 'serres', 'signorini', 'siphuncled', 'spacings', 'spilehole', 'stansfield', 'strik', 'subpectinate', 'subpleural', 'supersubtle', 'taifasy', 'tallula', 'telefoni', 'telemetry', 'temido', 'thaneland', 'therethrough', 'thwackingly', 'transcends', 'tribes', 'tripletail', 'undrawable', 'uninverted', 'unluxurious', 'unmist', 'unweeping', 'velzen', 'wabasha', 'watchen', 'wilenius', 'winterweed', 'woodly', 'wykoff', 'wysocki', 'xened', 'yokeage']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "totalLength = 0\n",
    "maxBucketLength = 0\n",
    "bucketWithLongestLength = 0\n",
    "minBucketLength = 99999999\n",
    "bucketwithMinLenth = 0\n",
    "\n",
    "# Go over all the hash buckets of the table and collect the statistics\n",
    "for num in tqdm(range(NUM_OF_HASH_BUCKETS)):\n",
    "    currentBucketLength = len(myHashTable[num])\n",
    "    totalLength += currentBucketLength\n",
    "    if currentBucketLength > maxBucketLength:\n",
    "        maxBucketLength = currentBucketLength\n",
    "        bucketWithLongestLength = num\n",
    "    #print(f'Run-{num} totalLength={totalLength} maxLength={maxBucketLength}')\n",
    "    if currentBucketLength < minBucketLength:\n",
    "        minBucketLength = currentBucketLength\n",
    "        bucketwithMinLenth = num\n",
    " \n",
    "avgBucketLength = totalLength / NUM_OF_HASH_BUCKETS\n",
    "print(f'Check TotalLength = {totalLength}')\n",
    "print(f'avgBucketLength = {avgBucketLength}')\n",
    "\n",
    "print(f'maxBucketLength = {maxBucketLength}')\n",
    "print(f'bucketWithLongestLength = {bucketWithLongestLength}')\n",
    "print(myHashTable[bucketWithLongestLength])\n",
    "\n",
    "print(f'minBucketLength = {minBucketLength}')\n",
    "print('bucketWithMinLength = ')\n",
    "print(myHashTable[bucketwithMinLenth])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b46134fc6bf11bbffd914b2f18eed44407e463932079c54ba3310810a6f5ffaf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
